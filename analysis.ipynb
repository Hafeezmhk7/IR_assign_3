{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4d4928",
   "metadata": {},
   "source": [
    "# Analysis (Chapter 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b6684",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "All datasets can be obtained by running the code below. The data will be sorted in the `./analysis_data/` folder. The format is the same as that of data you used in the previous part of this assignment, so make sure you change the dataset paths in the *assignment3.ipynb* to the correct data folder, except for the `common_words` stopwords. Also, set the RESET flag to `True`. Finally, make sure you dig around the dataset you are working with, to get familiar with the type of queries and documents, the type of relevance judgements, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c07750",
   "metadata": {},
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ir_datasets\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a884315",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19232dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2935bd",
   "metadata": {},
   "source": [
    "### Function to save data in the .tsv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f78517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tsv(\n",
    "    folder_name,\n",
    "    dataset_path,\n",
    "    concatenate_docs=False,\n",
    "    doc_text=None,\n",
    "    extract_partial=False,\n",
    "    extract=None,\n",
    "    provide_n_docs=False,\n",
    "    n_docs=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves queries, documents and qrels in asisgnment-specific .tsv format.\n",
    "\n",
    "    Input:\n",
    "        - dataset_path: dataset to be downloaded using ir_dataset package\n",
    "        - folder_name: name of the folder within the data_for_analysis folder\n",
    "    \"\"\"\n",
    "    # Check if the folder exists, and if not, create it\n",
    "    folder_path = \"analysis_data/\" + folder_name\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        dataset = ir_datasets.load(dataset_path)\n",
    "\n",
    "    # Queries\n",
    "    print(f\"Extracting queries of {folder_name} ...\")\n",
    "    query_path = os.path.join(folder_path, \"queries.tsv\")\n",
    "    if not os.path.exists(query_path):\n",
    "        total_queries = sum(1 for _ in dataset.queries_iter())\n",
    "        with open(query_path, \"w\", newline=\"\", encoding=\"utf-8\") as tsv_file:\n",
    "            tsv_writer = csv.writer(tsv_file, delimiter=\"\\t\")\n",
    "            for query in tqdm(\n",
    "                dataset.queries_iter(), total=total_queries, desc=\"Saving queries\"\n",
    "            ):\n",
    "                tsv_writer.writerow([query[0], query[1]])\n",
    "\n",
    "    # Docs\n",
    "    print(f\"Extracting documents of {folder_name} ...\")\n",
    "    doc_path = os.path.join(folder_path, \"collection.tsv\")\n",
    "    if not os.path.exists(doc_path):\n",
    "\n",
    "        total_docs = n_docs if provide_n_docs else sum(1 for _ in dataset.docs_iter())\n",
    "        sampled_docs = (\n",
    "            random.sample(list(dataset.docs_iter()), extract)\n",
    "            if extract_partial\n",
    "            else dataset.docs_iter()\n",
    "        )\n",
    "\n",
    "        with open(doc_path, \"w\", newline=\"\", encoding=\"utf-8\") as tsv_file:\n",
    "            tsv_writer = csv.writer(tsv_file, delimiter=\"\\t\")\n",
    "            for doc in tqdm(dataset.docs_iter(), total=total_docs, desc=\"Saving docs\"):\n",
    "                if concatenate_docs:\n",
    "                    tsv_writer.writerow(\n",
    "                        [doc[0], f\"{doc[doc_text[0]]} {doc[doc_text[1]]}\"]\n",
    "                    )\n",
    "                else:\n",
    "                    tsv_writer.writerow([doc[0], doc[1]])\n",
    "\n",
    "    # Qrels\n",
    "    print(f\"Extracting qrels of {folder_name} ...\")\n",
    "    ## Initialize file handles only if they don't exist\n",
    "    train_file_path = os.path.join(folder_path, \"train_pairs_graded.tsv\")\n",
    "    dev_file_path = os.path.join(folder_path, \"dev_pairs_graded.tsv\")\n",
    "    test_file_path = os.path.join(folder_path, \"test_pairs_graded.tsv\")\n",
    "\n",
    "    if not os.path.exists(train_file_path):\n",
    "        total_qrels = sum(1 for _ in dataset.qrels_iter())\n",
    "\n",
    "        ## Calculate the indices to split the data\n",
    "        index_80_percent = int(0.8 * total_qrels)\n",
    "        index_90_percent = int(0.9 * total_qrels)\n",
    "\n",
    "        with open(\n",
    "            os.path.join(folder_path, \"train_pairs_graded.tsv\"),\n",
    "            \"w\",\n",
    "            newline=\"\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as train_file, open(\n",
    "            os.path.join(folder_path, \"dev_pairs_graded.tsv\"),\n",
    "            \"w\",\n",
    "            newline=\"\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as dev_file, open(\n",
    "            os.path.join(folder_path, \"test_pairs_graded.tsv\"),\n",
    "            \"w\",\n",
    "            newline=\"\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as test_file:\n",
    "\n",
    "            train_writer = csv.writer(train_file, delimiter=\"\\t\")\n",
    "            dev_writer = csv.writer(dev_file, delimiter=\"\\t\")\n",
    "            test_writer = csv.writer(test_file, delimiter=\"\\t\")\n",
    "\n",
    "            for i, qrel in tqdm(\n",
    "                enumerate(dataset.qrels_iter()), total=total_qrels, desc=\"Saving qrels\"\n",
    "            ):\n",
    "                if i < index_80_percent:\n",
    "                    train_writer.writerow([qrel[0], qrel[1], qrel[2]])\n",
    "                elif i < index_90_percent:\n",
    "                    dev_writer.writerow([qrel[0], qrel[1], qrel[2]])\n",
    "                else:\n",
    "                    test_writer.writerow([qrel[0], qrel[1], qrel[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c69928",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_to_save = {\n",
    "    \"GENOMICS\": {\n",
    "        \"dataset_path\": \"medline/2004/trec-genomics-2005\",\n",
    "        \"concatenate_docs\": True,\n",
    "        \"doc_text\": [1, 2],\n",
    "    },\n",
    "    \"ARGS\": {\n",
    "        \"dataset_path\": \"argsme/2020-04-01/touche-2021-task-1\",\n",
    "        \"concatenate_docs\": True,\n",
    "        \"doc_text\": [3, 1],\n",
    "    },\n",
    "    \"GAMING\": {\n",
    "        \"dataset_path\": \"beir/cqadupstack/gaming\",\n",
    "        \"concatenate_docs\": True,\n",
    "        \"doc_text\": [2, 1],\n",
    "    },\n",
    "    \"NUTRITION\": {\n",
    "        \"dataset_path\": \"nfcorpus/train/nontopic\",\n",
    "        \"concatenate_docs\": True,\n",
    "        \"doc_text\": [2, 3],\n",
    "    },\n",
    "    \"CLIMATE\": {\n",
    "        \"dataset_path\": \"beir/climate-fever\",\n",
    "        \"concatenate_docs\": True,\n",
    "        \"doc_text\": [2, 1],\n",
    "    },\n",
    "    \"BUSINESS\": {\n",
    "        \"dataset_path\": \"beir/fiqa/train\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440731d",
   "metadata": {},
   "source": [
    "### Did you choose your scenario? Let's get the dataset! \n",
    "Please use the Dataset name as specified in the markdown cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a3142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scenario = input(\"What scenario are you interested in? \")\n",
    "while scenario not in datasets_to_save.keys():\n",
    "    print(\"Please specify a valid scenario.\")\n",
    "    scenario = input(\"What scenario are you interested in? \")\n",
    "\n",
    "print(f\"Saving {scenario} ...\")\n",
    "save_tsv(scenario, **datasets_to_save[scenario])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b747b7",
   "metadata": {},
   "source": [
    "## Comparing Pointwise, Pairwise and Listwise\n",
    "\n",
    "In the next few cells, we will compare the methods you've implemented. Helper functions are provided for you, which you can use to make some conclusions. You can modify the code as needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a971f",
   "metadata": {},
   "source": [
    "First, let's have a function that plots the average scores of relevant (levels 3 and 4) and non-relevant (levels 0, 1, and 2) scores in terms of training epochs for different loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics and models\n",
    "import json\n",
    "import torch\n",
    "from ltr.model import LTRModel\n",
    "\n",
    "N_FEATURES = 15\n",
    "\n",
    "pointwise_temp_model = LTRModel(N_FEATURES)\n",
    "pointwise_temp_model.load_state_dict(torch.load(\"./outputs/pointwise_model\"))\n",
    "\n",
    "pairwise_temp_model = LTRModel(N_FEATURES)\n",
    "pairwise_temp_model.load_state_dict(torch.load(\"./outputs/pairwise_spedup_model\"))\n",
    "\n",
    "listwise_temp_model = LTRModel(N_FEATURES)\n",
    "listwise_temp_model.load_state_dict(torch.load(\"./outputs/listwise_model\"))\n",
    "\n",
    "\n",
    "methods = [\n",
    "    {\"results_file\": \"./outputs/pointwise_res.json\", \"label\": \"Pointwise\"},\n",
    "    {\"results_file\": \"./outputs/pairwise_spedup_res.json\", \"label\": \"Pairwise\"},\n",
    "    {\"results_file\": \"./outputs/listwise_res.json\", \"label\": \"Listwise\"},\n",
    "]\n",
    "\n",
    "labels = []\n",
    "results = []\n",
    "q_results = []\n",
    "for m in methods:\n",
    "    labels.append(m[\"label\"])\n",
    "\n",
    "    with open(m[\"results_file\"]) as reader:\n",
    "        r = json.load(reader)\n",
    "\n",
    "    results.append(r[\"test_metrics\"])\n",
    "    q_results.append(r[\"test_query_level_metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3cf36",
   "metadata": {},
   "source": [
    "In the following cell, `compare_methods` and `plot_distribution` figures are imported. You can use the them for your analysis and observe how the different loss functions compare to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14492d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.utils import compare_methods\n",
    "\n",
    "compare_methods(labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.utils import plot_distribution\n",
    "\n",
    "plot_distribution(labels, q_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c075eed",
   "metadata": {},
   "source": [
    "1. Given the training results and evaluation results, please elaborate on the ranking performance with different loss functions.\n",
    "\n",
    "2. In this assignment, you extracted N_FEATURES features for each query-document pair. Now, consider adding 1-2 new features, and run the training again. Analyze training performance with the extended feature vectors.\n",
    "\n",
    "See the Canvas assignment for further details and to submit your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
