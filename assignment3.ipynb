{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b08a635cb01047dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from ltr.utils import seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Learning to Rank <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this assignment, we will implement several Learning to Rank (LTR) algorithms. We will start with offline LTR algorithms, which are trained on a dataset of precomputed features. We will then move to counterfactual LTR algorithms, which are trained on a dataset of user interactions.\n",
    "\n",
    "Table of contents:\n",
    "- [Chapter 1: Offline LRT](#o_LTR)\n",
    "    - [Section 1: Feature Extraction](#feature_extraction)\n",
    "    - [Section 2: Pointwise LRT](#pointwise_LTR)\n",
    "    - [Section 3: Pairwise LRT](#pairwise_LTR)\n",
    "    - [Section 4: Pairwise - Speed-up RankNet](#pairwise_speedup)\n",
    "    - [Section 5: Listwise LRT](#listwise_LTR)\n",
    "- [Chapter 2: Counterfactual LRT](#c_LTR)\n",
    "    - [Section 1: Utils](#utils)\n",
    "    - [Section 2: ListNet](#listnet)\n",
    "    - [Section 3: Unbiased ListNet](#unbiased_listnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef602d983baa9d90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# Chapter 1: Offline LTR <a class=\"anchor\" id=\"o_LTR\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9978e0796016b961",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A typical setup of learning to rank involves a feature vector constructed using a query-document pair, and a set of relevance judgements. In this assignment, you are given a small dataset that consists of number of queries with their corresponding list of documents. As the first step, you need to extract features for this dataset, and then use them in order to perform learninig-to-rank with different loss functions. In particular, the information about each of the files in this dataset is as follows:\n",
    "\n",
    "- `collection.tsv`: Each line consists of _document ID_ and the _document text_\n",
    "\n",
    "- `queries.tsv`: Each line consists of _query ID_ and the _query text_\n",
    "\n",
    "- `(train/dev/test)_pairs_graded.tsv`: Each line consists of _query ID_, _document ID_, _relevance score_; where relevance grade is an *ordinal* variable  with  5  grades,  for example: {`perfect`,`excellent`,`good`,`fair`,`bad`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d60b3e2cd8d41210",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 1: Feature Extraction <a class=\"anchor\" id=\"feature_extraction\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "The following cell contains the path of raw dataset files and a file with the list of stop words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: DO NOT CHANGE ANY OF THE FILE PATHS.\n",
    "\n",
    "COLLECTION_PATH = \"./data/collection.tsv\"\n",
    "QUERIES_PATH = \"./data/queries.tsv\"\n",
    "TRAIN_PATH = \"./data/train_pairs_graded.tsv\"\n",
    "DEV_PATH = \"./data/dev_pairs_graded.tsv\"\n",
    "TEST_PATH = \"./data/test_pairs_graded.tsv\"\n",
    "STOP_WORDS_PATH = \"./data/common_words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to preprocess query and document texts and extract term based statistics for documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.dataset import Queries, Preprocess\n",
    "\n",
    "prp = Preprocess(STOP_WORDS_PATH)\n",
    "\n",
    "queries = Queries(prp)\n",
    "queries.preprocess_queries(QUERIES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, you will use `Documents` class to extract document term-based stats, as this process might take longer, `if` statement and  `RESET` allows you to skip this step after the first time.\n",
    "\n",
    "Complete the implementation of `process_documents` function. you can find the implemenation in `ltr.dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Implement the function 'process_documents'\n",
    "\n",
    "from ltr.dataset import Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 96270it [00:00, 621680.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: DO NOT CHANGE DOC_JSON PATH\n",
    "DOC_JSON = \"./datasets/doc.pickle\"\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "RESET = True\n",
    "if os.path.exists(DOC_JSON) and not RESET:\n",
    "    with open(DOC_JSON, \"rb\") as file:\n",
    "        documents = pickle.load(file)\n",
    "else:\n",
    "    documents = Documents(prp)\n",
    "    documents.process_documents(COLLECTION_PATH)\n",
    "    with open(DOC_JSON, \"wb\") as file:\n",
    "        pickle.dump(documents, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `FeatureExtraction` in `ltr.dataset` is defined for extracting features. This class includes a class `extract` method, that will extract predefined features for each query-document pair.\n",
    "\n",
    "The other class, `GenerateFeatures` is already implemented to allow for reading lines of `(train\\dev\\test)_pairs_graded.tsv` files one by one, extract the features, and write them in the `(train\\dev\\test)_pairs_graded.tsv`. \n",
    "\n",
    "The `FeatureExtraction` class has the following property: `features` that is a `dict` with feature names as the keys and their values as dictionary values. The list of features that you have to implement can be found in `__feature_list__` inside `ltr.dataset` and also listed in `README.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: DO NOT CHANGE `args` VALUES FOR BM25 CALCULATION\n",
    "N_FEATURES = 15\n",
    "\n",
    "from ltr.dataset import FeatureExtraction, GenerateFeatures\n",
    "\n",
    "feature_ex = FeatureExtraction({}, documents, queries)\n",
    "\n",
    "feat_gen = GenerateFeatures(feature_ex)\n",
    "args = {}\n",
    "args[\"k1\"] = 1.5\n",
    "args[\"b\"] = 0.75\n",
    "args[\"idf_smoothing\"] = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo:**\n",
    "\n",
    "Implement method `extract` in `ltr.dataset.FeatureExtraction`. This method should return a dictionary with feature names as the keys and their values as dictionary values. This method is called inside `run` method of `GenerateFeatures` class. You can pass any required arguments through `args`. For instance, as defined in the previous cell, you can pass `k1` and `b` as BM25 arguments through `args`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_gen.run(TRAIN_PATH, TRAIN_PATH + \"g\", **args)\n",
    "feat_gen.run(DEV_PATH, DEV_PATH + \"g\", **args)\n",
    "feat_gen.run(TEST_PATH, TEST_PATH + \"g\", **args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataSet` class will read the generated feature files for train/valid/test splits and you can use them for training LTR models with different ranking loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: DO NOT CHANGE THE `fold_paths`\n",
    "\n",
    "from ltr.dataset import DataSet\n",
    "\n",
    "fold_paths = [\"./data/\"]\n",
    "num_relevance_labels = 5\n",
    "num_nonzero_feat = N_FEATURES\n",
    "num_unique_feat = N_FEATURES\n",
    "data = DataSet(\n",
    "    \"ir1-2023\", fold_paths, num_relevance_labels, num_unique_feat, num_nonzero_feat\n",
    ")\n",
    "\n",
    "data = data.get_data_folds()[0]\n",
    "data.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a79356db5683374",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of features: {data.num_features}\")\n",
    "# print some statistics\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"Split: {split}\")\n",
    "    split = getattr(data, split)\n",
    "    print(f\"\\tNumber of queries {split.num_queries()}\")\n",
    "    print(f\"\\tNumber of docs {split.num_docs()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b034476f52f28bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 1.2 Utility classes/methods\n",
    "\n",
    "You can use `LTRData` class in this assignment that is defined in `ltr.dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6be5d30fd0264dc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ltr.dataset import LTRData\n",
    "\n",
    "## example\n",
    "train_dl = DataLoader(LTRData(data, \"train\"), batch_size=32, shuffle=True)\n",
    "\n",
    "# this is how you would use it to quickly iterate over the train/val/test sets\n",
    "# - (of course, without the break statement!)\n",
    "for x, y in train_dl:\n",
    "    print(x.size(), y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a79c0f58db4af010",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can use `evaluate_model` from `ltr.eval` to evaluate a model, on a given split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66bc9b1a832d14d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ltr.eval import evaluate_model\n",
    "\n",
    "## example\n",
    "# function that scores a given feature vector e.g a network\n",
    "net = nn.Linear(N_FEATURES, 1)\n",
    "\n",
    "\n",
    "# the evaluate method accepts a function. more specifically, a callable (such as pytorch modules)\n",
    "def notwork(x):\n",
    "    return net(x)\n",
    "\n",
    "\n",
    "# evaluate the function\n",
    "_ = evaluate_model(data, notwork, split=\"validation\", print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66ae15ed8cb736b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell is used to generate reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df3d4a5ebf6dece6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use to get reproducible results\n",
    "from ltr.utils import seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a29483034efce729",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 2: Pointwise LTR <a class=\"anchor\" id=\"pointwise_LTR\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Let $x \\in \\mathbb{R}^d$ be an input feature vector, containing features for a query-document pair. Let $f: \\mathbb{R}^d \\rightarrow \\mathbb{R} $ be a function that maps this feature vector to a number $f(x)$ - either a relevance score (regression) or label (classification). The data $\\{x \\}$ are treated as feature vectors and the relevance judgements are treated as the target which we want to predict. \n",
    "\n",
    "In this section, you will implement a simple Pointwise model using either a regression loss, and use the train set to train this model to predict the relevance score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fdcb0b1bd78f6eda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 2.1: Neural Model\n",
    "\n",
    "\n",
    "\\# ToDO:\n",
    "Implement the following methods for the Class `LTRModel` in `ltr.model`:\n",
    "  - `__init__`\n",
    "  - `forward`\n",
    "\n",
    "We will use this neural network to learn a LTR model with different loss functions(Pointwise, Pairwise, Listwise), using the relevance grades as the label. This network should have the following attribute:\n",
    "  - `layers`: $N_{FEATURES} (input) \\rightarrow 10 \\rightarrow 1$ where each layer is a linear layer (`nn.Linear`) with a ReLu activation function (`nn.ReLU`) in between the layers. Use the default weight initialization scheme. (Hint: use `nn.Sequential` for a one-line forward function!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-917f63ec6b575f59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ltr.model import LTRModel\n",
    "\n",
    "# check the network configuration - layer dimensions and configurations\n",
    "point_nn_reg = LTRModel(data.num_features)\n",
    "print(point_nn_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d683efd6ca306e81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\\#ToDo: Implement regression loss in `pointwise_loss` function. You can find the definition in `ltr.ltr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-24edd9d567aac9da",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ltr.loss import pointwise_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0977a61ec0cfa7ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\\# ToDO:\n",
    "Implement `train_pointwise` function in `ltr.ltr` as a wrapper for training a pointwise LTR, that takes the model as input and trains the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import train_pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to test your code!\n",
    "pointwise_test_params = Namespace(epochs=1, lr=1e-3, batch_size=256, metrics={\"ndcg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a regression model for testing (we are only training for 1 epoch)\n",
    "met_reg = train_pointwise(point_nn_reg, pointwise_test_params, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-16ed543545863f61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The function `create_results` is defined in `ltr.utils` to create the results. It could be used  to generate your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb8314e4e579adac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPORTANT DO NOT CHANGE seed, params, or any of the PATH variables\n",
    "from ltr.utils import create_results\n",
    "\n",
    "seed(42)\n",
    "\n",
    "params_regr = Namespace(\n",
    "    epochs=11, lr=1e-3, batch_size=1, metrics={\"ndcg\", \"precision@05\", \"recall@05\"}\n",
    ")\n",
    "\n",
    "pointwise_regression_model = LTRModel(data.num_features)\n",
    "pw_regr = create_results(\n",
    "    data,\n",
    "    pointwise_regression_model,\n",
    "    train_pointwise,\n",
    "    pointwise_regression_model,\n",
    "    \"./outputs/pointwise_res.json\",\n",
    "    params_regr,\n",
    ")\n",
    "\n",
    "torch.save(pointwise_regression_model.state_dict(), \"./outputs/pointwise_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e48bb26c37eacea9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 3: Pairwise LTR <a class=\"anchor\" id=\"pairwise_LTR\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this section,  you will learn and implement RankNet, a  pairwise learning to rank algorithm.\n",
    "\n",
    "For a given query, consider two documents $D_i$ and $D_j$ with two different ground truth relevance  labels,  with  feature  vectors $x_i$ and $x_j$ respectively.   The  RankNet  model,  just  like  the pointwise model, uses $f$ to predict scores i.e $s_i=f(x_i)$ and $s_j=f(x_j)$, but uses a different loss during  training. $D_i \\triangleright D_j$ denotes  the  event  that $D_i$ should  be  ranked  higher  than $D_j$.   The  two outputs $s_i$ and $s_j$ are mapped to a learned probability that $D_i \\triangleright D_j$: \n",
    "\n",
    "\n",
    "$$        P_{ij} = \\frac{1}{1 + e^{-\\sigma(s_i - s_j)}} $$\n",
    "  \n",
    "where $\\sigma$ is a parameter that determines the shape of the sigmoid. The loss of the RankNet model is the cross entropy cost function:\n",
    "\n",
    "$$        C = - \\bar{P}_{ij} \\log P_{ij} - (1-\\bar{P}_{ij}) \\log (1 - P_{ij}) $$\n",
    "\n",
    "As the name suggests, in the pairwise approach to LTR, we optimize a loss $l$ over pairs of documents. Let $S_{ij} \\in \\{0, \\pm1 \\}$ be equal to $1$ if the relevance of document $i$ is greater than document $j$; $-1$ if document $j$ is more relevant than document $i$; and 0 if they have the same relevance. This gives us $\\bar{P}_{ij} = \\frac{1}{2} (1 + S_{ij})$ so that $\\bar{P}_{ij} = 1$ if $D_i \\triangleright D_j$; $\\bar{P}_{ij} = 0$ if $D_j \\triangleright D_i$; and finally $\\bar{P}_{ij} = \\frac{1}{2}$ if the relevance is identical. This gives us:\n",
    "\n",
    "$$        C = \\frac{1}{2}(1- S_{ij})\\sigma(s_i - s_j) + \\log(1+ e^{-\\sigma(s_i - s_j)}) $$\n",
    "\n",
    "Now, consider a single query for which $n$ documents have been returned. Let the output scores of the ranker be $s_j$ ; $j=\\{1, \\dots, n \\}$, the model parameters be $w_k \\in \\mathbb{R}^W$, and let the set of pairs of document indices used for training be $\\mathcal{P}$. Then, the total cost is $C_T = \\sum_{i,j \\in \\mathcal{P}} C(s_i; s_j)$. \n",
    "\n",
    "\n",
    "\n",
    "- Implement RankNet. You should construct training samples by creating all possible pairs of documents for a given query and optimizing the loss above.\n",
    "- Use the same model architecture as in pointwise LTR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5359ecd282448c2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the pairwise loss, we need to have a structured **dataloader** which detects the documents associated with a specific query. The class `QueryGroupedLTRData` is defined in `ltr.dataset` for this end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0009b5254fc5f2ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ltr.dataset import QueryGroupedLTRData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example\n",
    "train_data = QueryGroupedLTRData(data, \"train\")\n",
    "# this is how you would use it to quickly iterate over the train/val/test sets\n",
    "\n",
    "q_i = 300\n",
    "features_i, labels_i = train_data[q_i]\n",
    "print(f\"Query {q_i} has {len(features_i)} query-document pairs\")\n",
    "print(f\"Shape of features for Query {q_i}: {features_i.size()}\")\n",
    "print(f\"Shape of labels for Query {q_i}: {labels_i.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-acdb1bfcd2ec582e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#ToDO:\n",
    "\n",
    "First, implement the pairwaise loss in `pairwise_loss` function in `ltr.loss` as described above.\n",
    "Then, implement `train_pairwise` method in `ltr.train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a85b3e6ab896fd79",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ltr.loss import pairwise_loss\n",
    "from ltr.train import train_pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal training with pairwise loss requires a lot of time, you can investigate normal training of LTRModel with pairwise loss by uncommenting the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed(42)\n",
    "\n",
    "# params = Namespace(epochs=1, lr=1e-4, batch_size=1, metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "\n",
    "# pairwise_net = LTRModel(N_FEATURES)\n",
    "\n",
    "# create_results(\n",
    "#     data, pairwise_net,\n",
    "#     train_pairwise,\n",
    "#     pairwise_net,\n",
    "#     \"pairwise_normal_res.json\",\n",
    "#     params\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a95bb01f72fc76c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 4: Pairwise: Speed-up RankNet <a class=\"anchor\" id=\"pairwise_speedup\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "To speed up training of the previous model, we can consider a sped up version of the model, where instead of `.backward` on the loss, we use `torch.backward(lambda_i)`. \n",
    "\n",
    "The derivative of the total cost $C_T$ with respect to the model parameters $w_k$ is:\n",
    "\n",
    "$$        \\frac{\\partial C_T}{\\partial w_k} = \\sum_{(i,j) \\in \\mathcal{P}} \\frac{\\partial C(s_i, s_j)}{\\partial s_i} \\frac{\\partial s_i}{\\partial w_k} + \\frac{\\partial C(s_i, s_j)}{\\partial s_j} \\frac{\\partial s_j}{\\partial w_k} $$\n",
    "\n",
    "We can rewrite this sum by considering the set of indices $j$ , for which $\\{i,j\\}$ is a valid pair, denoted by $\\mathcal{P}_i$, and the set of document indices $\\mathcal{D}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_T}{\\partial w_k} = \\sum_{i \\in \\mathcal{D}}\n",
    "\\frac{\\partial s_i}{\\partial w_k} \\sum_{j \\in \\mathcal{P}_i} \n",
    "\\frac{\\partial C(s_i, s_j)}{\\partial s_i} \n",
    "$$\n",
    "\n",
    "This sped of version of the algorithm first computes scores $s_i$ for all the documents. Then for each $j= 1, \\dots, n$, compute:\n",
    "\n",
    "$$\n",
    "\\lambda_{ij} = \\frac{\\partial C(s_i, s_j)}{\\partial s_i} = \\sigma \\bigg( \\frac{1}{2}(1 - S_{ij}) -  \\frac{1}{1 + e^{\\sigma(s_i -s_j))}} \\bigg) \\\\\n",
    "\\lambda_i = \\sum_{j \\in \\mathcal{P}_i} \\frac{\\partial C(s_i, s_j)}{\\partial s_i} = \\sum_{j \\in \\mathcal{P}_i} \\lambda_{ij}\n",
    "$$\n",
    "\n",
    "That gives us:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_T}{\\partial w_k} = \\sum_{i \\in \\mathcal{D}}\n",
    "\\frac{\\partial s_i}{\\partial w_k} \\lambda_i\n",
    "$$\n",
    "\n",
    "This can be directly optimized in pytorch using: `torch.autograd.backward(scores, lambda_i)` \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a9b7b682a011642",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\\# ToDO:\n",
    "\n",
    "Implement the `compute_lambda_i` for the sped-up version of pairwise loss as described above. You can find the function definiton in `ltr.ltr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba7f8d8631e3f1d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ltr.loss import compute_lambda_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e7a6c3f6f5b8573d",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ltr.loss import mean_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-302ff24228d5d645",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\\# ToDO:\n",
    "\n",
    "Implement `train_pairwise_spedup` function for more efficient training with pairwise loss. You can find the definition in `ltr.train`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import train_pairwise_spedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT DO NOT CHANGE seed, params, or any of the PATH variables\n",
    "\n",
    "seed(42)\n",
    "\n",
    "params = Namespace(\n",
    "    epochs=11, lr=1e-3, batch_size=1, metrics={\"ndcg\", \"precision@05\", \"recall@05\"}\n",
    ")\n",
    "\n",
    "sped_up_pairwise_model = LTRModel(N_FEATURES)\n",
    "\n",
    "create_results(\n",
    "    data,\n",
    "    sped_up_pairwise_model,\n",
    "    train_pairwise_spedup,\n",
    "    sped_up_pairwise_model,\n",
    "    \"./outputs/pairwise_spedup_res.json\",\n",
    "    params,\n",
    ")\n",
    "\n",
    "torch.save(sped_up_pairwise_model.state_dict(), \"./outputs/pairwise_spedup_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section 5: Listwise LTR <a class=\"anchor\" id=\"listwise_LTR\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this section, you will implement LambdaRank, a listwise approach to LTR. Consider the computation of $\\lambda$ for sped-up RankNet (that you've already implemented). $\\lambda$ here amounts to the 'force' on a document given its neighbours in the ranked list. The design of $\\lambda$ in LambdaRank is similar to RankNet, but is scaled by DCG gain from swapping the two documents in question. Let's suppose that the corresponding ranks of doucment $D_i$ and $D_j$ are $r_i$ and $r_j$ respectively. Given a ranking measure $IRM$, such as $NDCG$ or $ERR$, the lambda function in LambdaRank is defined as:\n",
    "\n",
    "\n",
    "$$        \\frac{\\partial C}{\\partial s_i} = \\sum_{j \\in D} \\lambda_{ij} \\cdot |\\bigtriangleup IRM (i,j)| $$\n",
    "\n",
    "Where $|\\bigtriangleup IRM(i,j)|$ is the absolute difference in $IRM$ after swapping the rank positions $r_i$ and $r_j$ while leaving everything else unchanged ($| \\cdot |$ denotes the absolute value). Note that we do not backpropogate $|\\bigtriangleup IRM|$, it is treated as a constant that scales the gradients. In this assignment we will use $|\\bigtriangleup NDCG|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-351c194e6797d0a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\\# ToDO:\n",
    "\n",
    "Implement the listwise loss in `listwise_loss` function defined in `ltr.ltr`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.loss import listwise_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `mean_lambda_list` is defined in `ltr.ltr`.\n",
    "\n",
    "\\# ToDo:\n",
    "Implement `train_listwise` function to train with list_wise loss. you can find the definition in `ltr.train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-59d3cccadbb8acae",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from ltr.loss import mean_lambda_list\n",
    "from ltr.train import train_listwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT DO NOT CHANGE seed, params, or any of the PATH variables\n",
    "\n",
    "seed(42)\n",
    "\n",
    "params = Namespace(\n",
    "    epochs=11, lr=1e-4, batch_size=1, metrics={\"ndcg\", \"precision@05\", \"recall@05\"}\n",
    ")\n",
    "\n",
    "listwise_model = LTRModel(N_FEATURES)\n",
    "\n",
    "create_results(\n",
    "    data,\n",
    "    listwise_model,\n",
    "    train_listwise,\n",
    "    listwise_model,\n",
    "    \"./outputs/listwise_res.json\",\n",
    "    params,\n",
    ")\n",
    "\n",
    "torch.save(listwise_model.state_dict(), \"./outputs/listwise_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Chapter 2: Counterfactual LTR <a class=\"anchor\" id=\"c_LTR\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, you will implement counterfactual learning to rank algorithms. In contrast to offline LTR algorithms, counterfactual LTR algorithms are trained on a dataset of user interactions. First, we will load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.dataset import load_data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that there is a logging policy that shows the results for each query to the users and logs the user clicks.\n",
    "For that, we provide a logging policy simulator `LoggingPolicy`.\n",
    "Our logging policy only shows top 20 documents to the users.\n",
    "You can use this simulator to:\n",
    "- Get the position of the documents for a query in the SERP: `query_positions`.\n",
    "- Gather the (simulated) clicks of users for a query: `gather_clicks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.logging_policy import LoggingPolicy\n",
    "\n",
    "logging_policy = LoggingPolicy()\n",
    "\n",
    "# Gather the clicks on the SERP for query 20\n",
    "for i in range(10):\n",
    "    clicked_docs = np.where(logging_policy.gather_clicks(20))[0]\n",
    "    clicked_positions = logging_policy.query_positions(20)[clicked_docs]\n",
    "    print(\n",
    "        f\"clicks for session {i+1} on documents\",\n",
    "        clicked_docs,\n",
    "        \"on positions\",\n",
    "        clicked_positions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Utils <a class=\"anchor\" id=\"utils\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "### Click data loader\n",
    "First, we need to have a data loader that feeds the model with features and click data.\n",
    "In this data loader, you have to select `topk=20` items for each query, and return three tensors:\n",
    "- Feature vectors of the selected documents,\n",
    "- One instance of the clicks over the selected documents, using the `gather_clicks(qid)` function, and\n",
    "- The positions of the selected documents in the SERP.\n",
    "\n",
    "**IMPORTANT** Here you *should not* use the `labels` for training. It is assumed that we cannot observe the real labels and want to use the `clicks` to train our LTR model \n",
    "instead.\n",
    "\n",
    "ToDo: Implement the `__getitem__` method in `ClickLTRData` class in `ltr.dataset` to return the feature vectors, clicks, and positions of the selected documents for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.dataset import ClickLTRData\n",
    "\n",
    "train_dl = DataLoader(ClickLTRData(data, logging_policy), batch_size=1, shuffle=True)\n",
    "\n",
    "for features, clicks, positions in train_dl:\n",
    "    print(features.shape, clicks.shape, positions.shape)\n",
    "\n",
    "    assert positions.dtype == torch.long\n",
    "    print(features.shape, clicks.shape, positions.shape)\n",
    "    print(\"clicks:\", clicks)\n",
    "    print(\"positions:\", positions)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTR model\n",
    "Further, let's copy the idea of the `LTRModel` from previous chapter to implement `CLTRModel` in `ltr.model` for counterfactual learning to rank and take the width of the middle layer as an argument.\n",
    "\n",
    "ToDo: Implement the `__init__` and `forward` methods in `CLTRModel` class in `ltr.model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.model import CLTRModel\n",
    "\n",
    "net = CLTRModel(data.num_features, width=20)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: ListNet <a class=\"anchor\" id=\"listnet\"></a>\n",
    "\n",
    "In the previous chapter, you have implemented different loss functions for LTR.\n",
    "Here we use another well known listwise loss funtion, called `ListNet`, and will use it for our unbiased LTR model.\n",
    "The idea behind ListNet is very simple:\n",
    "To solve the discontinuity issue of NDCG, in **ListNet**, the loss function is based on probability distribution on permutations.\n",
    "\n",
    "Define a family of distributions on permutation of scores $z$, $P_z(\\pi)$, s.t. $\\sum_{\\pi\\in\\Omega} P_z(\\pi)=1$, where $\\Omega$ is the set of all $n!$ permutations.\n",
    "Ideally, we want the scores of our LTR model lead to the same permutation distribution as the labels $y$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min KL(P_y,P_z)=-\\sum_{\\pi\\in\\Omega} P_y(\\pi) \\log P_z(\\pi)\n",
    "$$\n",
    "\n",
    "Plackett-Luce distribution gives a general formula for calculating the permutation distribution:\n",
    "\n",
    "$$\n",
    "P_z(\\pi) = \\prod_{j=1}^{n} \\frac{\\exp(z_{\\pi(j)})}{\\sum_{k=j}^{n} \\exp(z_{\\pi(k)})}\n",
    "$$\n",
    "In ListNet, instead of calculating $n!$ permutation probabilities, the top one probability of each document is calculated:\n",
    "\n",
    "$$\n",
    "P_z(j) = \\sum_{\\pi(1)=j, \\pi\\in\\Omega} P_z(\\pi) = \\frac{\\exp(z_{j})}{\\sum_{k=1}^{n} \\exp(z_{k})},\n",
    "$$\n",
    "which is the softmax function.\n",
    "\n",
    "Then, the loss is defined as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{ListNet}}=-\\sum_{j=1}^{n} P_y(j) \\log P_z(j),\n",
    "$$\n",
    "where the softmax function is used to calculate $P_y(j)$ and $P_z(j)$ from the labels and predictions, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ListNet loss function\n",
    "\n",
    "ToDo: Implement the ListNet loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.loss import listNet_loss\n",
    "\n",
    "biased_net = CLTRModel(data.num_features, width=20)\n",
    "\n",
    "for features, clicks, positions in train_dl:\n",
    "    print(features.shape, clicks.shape, positions.shape)\n",
    "    output = biased_net(features)\n",
    "    print(output.shape, clicks.shape)\n",
    "    loss = listNet_loss(output, clicks)\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biased ListNet training\n",
    "Now use `listNet_loss` to train an LTR model. Since we use `clicks` instead of `relevance`, and do not correct for the bias, this would be a biased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import train_biased_listNet\n",
    "\n",
    "params = Namespace(\n",
    "    epochs=1, lr=1e-4, batch_size=1, metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"}\n",
    ")\n",
    "\n",
    "biased_net = CLTRModel(15, width=20)\n",
    "train_biased_listNet(biased_net, params, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "Since we randomly simulate clicks and use them to train our model, for the evaluation we train and save 10 different models and inspect the average and std over them.\n",
    "\n",
    "**IMPORTANT** Run the following cell to store your models and results. After it finishes, make sure to push the results to the git repo.\n",
    "\n",
    "_Estimated time on Codespaces_: 5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ltr.utils import create_results\n",
    "# from ltr.train import train_biased_listNet\n",
    "\n",
    "# seed(42)\n",
    "# params = Namespace(\n",
    "#     epochs=20, lr=1e-4, batch_size=1, metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"}\n",
    "# )\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(\"Training Model\", i)\n",
    "#     biased_net = CLTRModel(15, width=20)\n",
    "#     create_results(\n",
    "#         data,\n",
    "#         biased_net,\n",
    "#         train_biased_listNet,\n",
    "#         biased_net,\n",
    "#         f\"./outputs/biased_listNet_{i}.json\",\n",
    "#         params,\n",
    "#     )\n",
    "\n",
    "#     torch.save(biased_net.state_dict(), f\"./outputs/biased_listNet_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Unbiased ListNet <a class=\"anchor\" id=\"unbiased_listnet\"></a>\n",
    "\n",
    "### Unbiased ListNet loss function\n",
    "\n",
    "Now, we use IPS to have an unbiased ListNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.loss import unbiased_listNet_loss\n",
    "\n",
    "unbiased_net = CLTRModel(data.num_features, width=20)\n",
    "propensity = logging_policy.propensity\n",
    "\n",
    "for features, clicks, positions in train_dl:\n",
    "    print(features.shape, clicks.shape, positions.shape)\n",
    "    output = biased_net(features)\n",
    "    print(output.shape, clicks.shape)\n",
    "    loss = unbiased_listNet_loss(output, clicks, propensity[positions.data.numpy()])\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbiased ListNet training\n",
    "Now use `unbiased_listNet_loss` to train an LTR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import train_unbiased_listNet\n",
    "\n",
    "params = Namespace(\n",
    "    epochs=1,\n",
    "    lr=1e-4,\n",
    "    batch_size=1,\n",
    "    propensity=logging_policy.propensity,\n",
    "    metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"},\n",
    ")\n",
    "\n",
    "biased_net = CLTRModel(15, width=20)\n",
    "train_unbiased_listNet(biased_net, params, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results\n",
    "Similar to the biased model, here we train 10 different unbiased models and save them to inspect the average and std over them.\n",
    "\n",
    "**IMPORTANT** Run the following cell to store your models and results. After it finishes, make sure to push the results to the git repo.\n",
    "\n",
    "_Estimated time on Codespaces_: 5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ltr.utils import create_results\n",
    "# from ltr.train import train_unbiased_listNet\n",
    "\n",
    "# seed(42)\n",
    "# params = Namespace(\n",
    "#     epochs=20,\n",
    "#     lr=1e-4,\n",
    "#     batch_size=1,\n",
    "#     propensity=logging_policy.propensity,\n",
    "#     metrics={\"ndcg@10\", \"precision@10\", \"recall@10\"},\n",
    "# )\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(\"Training Model\", i)\n",
    "#     unbiased_net = CLTRModel(15, width=20)\n",
    "#     create_results(\n",
    "#         data,\n",
    "#         unbiased_net,\n",
    "#         train_unbiased_listNet,\n",
    "#         unbiased_net,\n",
    "#         f\"./outputs/unbiased_listNet_{i}.json\",\n",
    "#         params,\n",
    "#     )\n",
    "\n",
    "#     torch.save(unbiased_net.state_dict(), f\"./outputs/unbiased_listNet_{i}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "ir_assignment3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
